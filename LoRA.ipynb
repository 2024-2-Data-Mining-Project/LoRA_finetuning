{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rank, alpha, dropout 설정은 더 찾아봐야 해요\n",
    "\n",
    "> Epoch설정도 validation이랑 같이 돌려서 설정해야 함요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# KoBERT 모델과 토크나이저 로드\n",
    "model = BertModel.from_pretrained('monologg/kobert').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 모든 파라미터 동결\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRA 레이어 정의\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, input_dim, rank=8, alpha=16, dropout=0.05):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.down_project = nn.Linear(input_dim, rank, bias=False)\n",
    "        self.up_project = nn.Linear(rank, input_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(p=dropout)  # Dropout 추가\n",
    "\n",
    "        # Scaling Factor 추가\n",
    "        self.alpha = alpha\n",
    "        self.scaling = self.alpha / rank  # 스케일링 계산\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down_project(x)  # 차원 축소\n",
    "        x = self.dropout(x)       # Dropout 적용\n",
    "        return self.scaling * self.up_project(x)  # 스케일링 및 차원 복원\n",
    "\n",
    "# LoRA 레이어 초기화\n",
    "lora_layer = LoRALayer(input_dim=768, rank=16).to(device)\n",
    "\n",
    "# 데이터셋 정의\n",
    "class LiteraryMLMDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512, mask_prob=0.15):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "\n",
    "        # MLM: 마스킹 생성\n",
    "        labels = input_ids.clone()\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "        mask_arr = (rand < self.mask_prob) * (input_ids != self.tokenizer.cls_token_id) * \\\n",
    "                   (input_ids != self.tokenizer.sep_token_id) * (input_ids != self.tokenizer.pad_token_id)\n",
    "\n",
    "        # 마스킹된 위치를 [MASK] 토큰으로 대체\n",
    "        input_ids[mask_arr] = self.tokenizer.mask_token_id\n",
    "\n",
    "        # 마스크된 위치의 정답은 그대로 남기고, 나머지는 -100으로 설정 (loss 계산 제외)\n",
    "        labels[~mask_arr] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids.to(device),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze().to(device),\n",
    "            'labels': labels.to(device)\n",
    "        }\n",
    "\n",
    "# DataLoader 생성\n",
    "texts = df['origin_text_list'].tolist()\n",
    "dataset = LiteraryMLMDataset(texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# 옵티마이저 설정 (LoRA 레이어만 학습)\n",
    "optimizer = torch.optim.AdamW(lora_layer.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# 학습 루프\n",
    "model.train()\n",
    "lora_layer.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1} Progress\"):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # 프리트레인된 KoBERT 모델에서 임베딩 추출\n",
    "        with torch.no_grad():  # KoBERT의 파라미터는 업데이트하지 않음\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pretrained_embedding = outputs.last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "        # LoRA 레이어 통과\n",
    "        lora_embedding = lora_layer(pretrained_embedding)\n",
    "\n",
    "        # 프리트레인된 임베딩 +  LoRA 임베딩\n",
    "        combined_embedding = pretrained_embedding + lora_embedding\n",
    "\n",
    "        # 예측 layer에 위에서 계산된 임베딩 삽입\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        prediction_layer = nn.Linear(combined_embedding.size(-1), vocab_size).to(device)\n",
    "        predictions = prediction_layer(combined_embedding)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fn(predictions.view(-1, vocab_size), labels.view(-1))\n",
    "\n",
    "        # 역전파 및 옵티마이저 단계\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
